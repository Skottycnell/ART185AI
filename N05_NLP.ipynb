{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2019 Fabian Offert, [MIT License](LICENSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "- Understanding LSTM Networks: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Karpathy's char-rnn: https://github.com/karpathy/char-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using the Gensim and SpaCy NLP libraries that provide high-level interfaces for a lot of common NLP tasks, in addition to some basic system librarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim # conda install gensim\n",
    "import spacy # conda install -c conda-forge spacy\n",
    "\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "from collections import *\n",
    "import math\n",
    "\n",
    "nlp = spacy.load('en') # python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "You should have received the Proust dataset by email. Additionally, the Shakespeare dataset that Andrej Karpathy uses in his article can be downloaded from his site: http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt . The pre-trained Google News word embeddings can be downloaded from [the original word2vec site](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.\n",
    "- What is being learned? The probability of a word appearing in a fixed window around another word.\n",
    "- The neural network is trained on word pairs, but the hidden weights are actually used.\n",
    "- Two flavors: bag-of-words, skip-gram\n",
    "- Works well with very large corpora (109+ words)\n",
    "- Can be used to predict structural mappings, i.e. analogies. “Classic” task: man is to king = woman is to?\n",
    "\n",
    "![](img/embeddings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings (Google News corpus, $10^{10}$ words)\n",
    "\n",
    "Using pre-trained embeddings with Gensim is as simple as one line of code. See the [Gensim word2vec documentation](https://radimrehurek.com/gensim/models/word2vec.html) for a list integrated vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C binary format\n",
    "wv_news = gensim.models.KeyedVectors.load_word2vec_format('sources/google300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format that Gensim requires for analogy questions is a bit confusing. It is derived from the actual (arithmetical) vector operation, where `king - man + woman = ?`, which is why woman and king are the \"positive\" terms and man is the \"negative\" term. Below, we transcribe it to more intuitive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'man'\n",
    "# is to\n",
    "b = 'king'\n",
    "# like \n",
    "c = 'woman'\n",
    "# is to ?\n",
    "\n",
    "wv_news.wv.most_similar(positive=[c, b], negative=[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the wonderful [TensorBoard embedding projector](https://projector.tensorflow.org/) to visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "words = []\n",
    "\n",
    "vectors = np.zeros((len(wv_news.wv.vocab), dim))\n",
    "for i, word in enumerate(wv_news.wv.index2word):\n",
    "    vectors[i] = wv_news.wv[word]\n",
    "    words.append(word)\n",
    "\n",
    "with open('data.tsv', 'w+') as f:\n",
    "    for vector in vectors.tolist():\n",
    "        for point in vector:\n",
    "            f.write(str(point) + '\\t')\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('metadata.tsv', 'w+') as f:\n",
    "    for word in words:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-trained embeddings (\"In Search of Lost Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In Search of Lost Time\" is all about [links and similarities](https://en.wikipedia.org/wiki/Involuntary_memory): between times, places, things, senses, and people. Its arguably most famous scene is the \"Madeleine\" passage, where the experience of eating a simple [French coffee cake](https://en.wikipedia.org/wiki/Madeleine_(cake)) leads the narrator to remember a childhood episode and, subsequently, his whole childhood and youth. How can we explore these links and similarities computationally? With word embeddings, of course.\n",
    "\n",
    "![](https://marimann.files.wordpress.com/2012/02/proust_16anni_nadar.jpg)\n",
    "\n",
    "> And suddenly the memory returns. The taste was that of the little crumb of madeleine which on Sunday mornings at Combray (because on those mornings I did not go out before church-time), when I went to say good day to her in her bedroom, my aunt Leonie used to give me, dipping it first in her own cup of real or of lime-flower tea. The sight of the little madeleine had recalled nothing to my mind before I tasted it; perhaps because I had so often seen such things in the interval, without tasting them, on the trays in pastry-cooks' windows, that their image had dissociated itself from those Combray days to take its place among others more recent; perhaps because of those memories, so long abandoned and put out of mind, nothing now survived, everything was scattered; the forms of things, including that of the little scallop-shell of pastry, so richly sensual under its severe, religious folds, were either obliterated or had been so long dormant as to have lost the power of expansion which would have allowed them to resume their place in my consciousness. But when from a long-distant past nothing subsists, after the people are dead, after the things are broken and scattered, still, alone, more fragile, but with more vitality, more unsubstantial, more persistent, more faithful, the smell and taste of things remain poised a long time, like souls, ready to remind us, waiting and hoping for their moment, amid the ruins of all the rest; and bear unfaltering, in the tiny and almost impalpable drop of their essence, the vast structure of recollection.\n",
    "And once I had recognized the taste of the crumb of madeleine soaked in her decoction of lime-flowers which my aunt used to give me (although I did not yet know and must long postpone the discovery of why this memory made me so happy) immediately the old grey house upon the street, where her room was, rose up like the scenery of a theatre to attach itself to the little pavilion, opening on to the garden, which had been built out behind it for my parents (the isolated panel which until that moment had been all that I could see); and with the house the town, from morning to night and in all weathers, the Square where I was sent before luncheon, the streets along which I used to run errands, the country roads we took when it was fine. And just as the Japanese amuse themselves by filling a porcelain bowl with water and steeping in it little crumbs of paper which until then are without character or form, but, the moment they become wet, stretch themselves and bend, take on colour and distinctive shape, become flowers or houses or people, permanent and recognisable, so in that moment all the flowers in our garden and in M. Swann's park, and the water-lilies on the Vivonne and the good folk of the village and their little dwellings and the parish church and the whole of Combray and of its surroundings, taking their proper shapes and growing solid, sprang into being, town and gardens alike, from my cup of tea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim thankfully allows us to pass a Python generator as input to the word2vec model, which allows us to read the very large corpus sentence by sentence, instead of reading it into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            for sentence in paragraph.split('.'):\n",
    "                \n",
    "                # Use only lower case\n",
    "                sentence = sentence.lower()\n",
    "\n",
    "                # Remove all punctuation\n",
    "                exclude = set(string.punctuation)\n",
    "                sentence = ''.join(char for char in sentence if char not in exclude)\n",
    "\n",
    "                # Remove whitespaces\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                # Line as list\n",
    "                sentence = sentence.split()\n",
    "                \n",
    "                # Only return non-empty lines\n",
    "                if len(sentence) > 0: yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model is general enough to answer the standard analogy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.840269923210144),\n",
       " ('laundress', 0.8244017958641052),\n",
       " ('historian', 0.82276451587677),\n",
       " ('patronage', 0.8200148344039917),\n",
       " ('jardin', 0.8188639879226685),\n",
       " ('south', 0.808866560459137),\n",
       " ('balloon', 0.8062515258789062),\n",
       " ('manager', 0.8060844540596008),\n",
       " ('painter', 0.803223729133606),\n",
       " ('ambassador', 0.799126148223877)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = yield_file('sources/proust_ascii.txt') \n",
    "wv_proust = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# wv_proust.wv.accuracy('7-nlp/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is, which is good news for our further investigation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Embeddings with named entity recognition\n",
    "\n",
    "We would like to extract some more semantic information from our Proust model, namely we would like to see if character relations are preserved in vector space. To do that, we will run named entity recognition before building the model. SpaCy comes with a built-in named entity recognizer that we are using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file_tagged(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def _tag_word(self, word):\n",
    "        text = word.text\n",
    "        if word.ent_type_: tag = word.ent_type_\n",
    "        else: tag = word.pos_\n",
    "        return text + '|' + tag\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            # SpaCy magic\n",
    "            doc = nlp(paragraph)\n",
    "    \n",
    "            # Detect and merge entitites\n",
    "            for ent in doc.ents:\n",
    "                ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.root.ent_type_)\n",
    "    \n",
    "            # Detect and merge noun chunks\n",
    "            for nc in doc.noun_chunks:\n",
    "                while len(nc) > 1 and nc[0].dep_ not in ('advmod', 'amod', 'compound'):\n",
    "                    nc = nc[1:]\n",
    "                nc.merge(tag=nc.root.tag_, lemma=nc.text, ent_type=nc.root.ent_type_)\n",
    "            \n",
    "            for sentence in doc.sents:\n",
    "                words = []\n",
    "                for word in sentence:\n",
    "                    if not word.is_space: \n",
    "                        words.append(self._tag_word(word))\n",
    "                            \n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = yield_file_tagged('sources/proust_ascii.txt') \n",
    "wv_proust_tagged = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust_tagged.save('wv_proust_tagged.gensimmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test case here is a hard one. At the very end of the whole book, it becomes apparent that M. de Charlus had romantic relationships with both Morel and Marcel's friend Bloch. We will investigate if this is preserved in the vector space by checking it against one of the more prominent romantic relations, the relation between Marcel and Albertine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10617, size=300, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Morel|PERSON', 0.8458129167556763),\n",
       " ('Bloch|PERSON', 0.8372248411178589),\n",
       " ('Saint-Loup|ORG', 0.8185210824012756),\n",
       " ('Swann|PERSON', 0.8098665475845337),\n",
       " ('M. de Norpois|ORG', 0.8037635087966919),\n",
       " ('M. de Guermantes|PERSON', 0.7948862910270691),\n",
       " ('Elstir|PERSON', 0.7704919576644897),\n",
       " ('Robert|PERSON', 0.765677809715271),\n",
       " ('Odette|PROPN', 0.7484169006347656),\n",
       " ('Cottard|PERSON', 0.7473478317260742)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_proust_tagged_reloaded = gensim.models.Word2Vec.load('wv_proust_tagged.gensimmodel')\n",
    "print(wv_proust_tagged_reloaded)\n",
    "wv_proust_tagged_reloaded.wv.most_similar(positive=['Albertine|PERSON', 'M. de Charlus|PERSON'], negative=['I|PRON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory and Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information: Hartley/Nyquist/Shannon\n",
    "\n",
    "Watch: https://www.youtube.com/watch?v=2s3aJfRr9gE\n",
    "\n",
    "- Quadruplex telegraph: different-strength and different-direction currents: +1V, -1V, +5V, -5V\n",
    "- Resolution is limited by electrical noise\n",
    "- Problem: electrical pulses bleed into each other beyond a certain pulse rate: [intersymbol interference](https://en.wikipedia.org/wiki/Intersymbol_interference), for digital systems: [Nyquist–Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)\n",
    "- Based on these fundamental physical limitations, we can define the [channel capacity](https://en.wikipedia.org/wiki/Channel_capacity) by means of the possible rate of symbols per unit of time $n$ and the possible differences per symbol $s$\n",
    "- This generates a decision tree with $s^n$ leaves, where the number of leaves/base of he tree is the size of *message space*\n",
    "- Given a message composed of symbols from this message space, how many questions per symbol do I have to ask at minimum to guess the content of a message. E.g., for a symbol space of size $26$ (the alphabet), I could ask: \"Is it less than N?\". If it is: \"Is it less than G?\", etc.. I will need minimum 4, and maximum 5 questions to be 100% certain of the sent symbol.\n",
    "- In general, $2^{\\text{questions}} = \\text{message space}$\n",
    "- It follows, that, on average, I will need $x = \\log_2(26) \\approx 4.7$ questions to guess one symbol correctly.\n",
    "- Hartley (1928): *information* of a mesage $H = n \\log2(s)$, where $n$ is the number of symbols in the message, and $s$ is the number of different symbols available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence/independence: Markov\n",
    "\n",
    "Watch: https://www.youtube.com/watch?v=WyAtOqfCiBw and https://www.youtube.com/watch?v=Ws63I3F7Moc\n",
    "\n",
    "- Bernoulli's [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers): convergence of results of a trial on the [expected value](https://en.wikipedia.org/wiki/Expected_value) as the number of trials approaches infinity\n",
    "- Generally: for large numbers of random trials, things converge on averages, and the probability of variation away from averages forms predictable distributions ([central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem))\n",
    "- Nekrasov: independent variables are a necessary condition for the law of large numbers\n",
    "- Markov: law of large numbers also applies for dependent variables, as demonstrated by [Markov chains](https://en.wikipedia.org/wiki/Markov_chain): *states* and *transition matrices* introduce *short-term memory*\n",
    "- Examples: https://en.wikipedia.org/wiki/Examples_of_Markov_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximations to English: Shannon\n",
    "\n",
    "- Shannon, in \"A Mathematical Theory of Communication\" (1948), famoulsy defines communication as selection:\n",
    "> \"The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design.\"\n",
    "- We can model such systems - including a natural language like English - with Markov chains:\n",
    "> \"We can also approximate to a natural language by means of a series of simple artificial languages. The zero-order approximation is obtained by choosing all letters with the same probability and independently. The first-order approximation is obtained by choosing successive letters independently but each letter having the same probability that it has in the natural language. 5 Thus, in the first-order approximation to English, E is chosen with probability. 12 (its frequency in normal English) and W with probability .02, but there is no influence between adjacent letters and no tendency to form the preferred digrams such as TH, ED, etc. In the second-order approximation, digram structure is introduced. After a letter is chosen, the next one is chosen in accordance with the frequencies with which the various letters follow the first one. This requires a table of digram frequencies [...]. In the third-order approximation, trigram structure is introduced. Each letter is chosen with probabilities which depend on the preceding two letters.\"\n",
    "- Why does this work? Exactly because the transition probabilities converge, given that we have enough \"trials\", on a reasonable distribution, from which we can build a transition matrix\n",
    "- How do we measure the information of such a process? Shannon:\n",
    "> \"We have represented a discrete information source as a Markoff process. Can we define a quantity which will measure, in some sense, how much information is \"produced\" by such a process, or better, at what rate information\n",
    "is produced?\"\n",
    "- This measure is [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))!\n",
    "- Entropy is defined as $H = \\sum_{i=1}^{n}p_i \\log(\\frac{1}{p_i})$\n",
    "- It is based on the *uncertainty* of a fair coin flip: both outcomes are equally likely, the entropy is $0.5 \\log(\\frac{1}{0.5}) + 0.5 \\log(\\frac{1}{0.5}) = 0.5+0.5 = 1$. Generally, entropy is maximum where all outcomes are equally likely.\n",
    "\n",
    "![](img/shannon.jpg)\n",
    "\n",
    "- This can again be illustrated by the average number of questions I need to ask to find a symbol\n",
    "- Suppose we have a machine (a \"discrete informations source represented as a Markov process\") that generates letters from the alphabet A,B,C, and D with $P=(0.5|0.25|0.125|0.125)$. To find the next letter I could ask \"Is it A?\". Because it is more likely that it actually *is* A, we are done in 50% of the time with just one question. If it is not A, then we can ask: \"Is it B?\", and we will be right 75% of the time (because we already excluded the possibility of it being A). Again, we build a decision tree, just with \"weights\", i.e. probabilities, attached to its branches, where the height of the decision tree represents the maximum number of questions we need to ask to find the next letter.\n",
    "- The total entropy of the source is the entropy of each state times the likelihood of that state.\n",
    "- Entropy has very important implications for compression. For instance, with [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) (lossless compression), the limit of compression is the entropy of the message source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Proust from scratch with an unsmoothed maximum likelihood character level language (Markov) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for entropy calculation: fair coin toss\n",
    "print(2 * (0.5 * math.log((1/0.5), 2)))\n",
    "\n",
    "# Sanity check for entropy calculation: zero order, 4-letter alphabet\n",
    "H = 0\n",
    "for i in range(4):\n",
    "    Hij = 0\n",
    "    Pi = 0.25\n",
    "    for pij in range(4):\n",
    "        Hij += 0.25 * math.log((1/0.25), 2)\n",
    "    H +=  Pi * Hij\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov():\n",
    "    \n",
    "    def __init__(self, filename, order=4, word=False, train_test_split=0.1):\n",
    "        # Read the whole corpus into memory\n",
    "        self.order = order\n",
    "        self.word = word\n",
    "        data = open(filename).read()\n",
    "        \n",
    "        # If this is a word model, we will operate on a list - almost all functions still work because\n",
    "        # strings are just lists of characters!\n",
    "        if self.word: \n",
    "            data = data.split()\n",
    "        \n",
    "        # Split in train and test data\n",
    "        test_data_size = math.floor(len(data)*train_test_split)\n",
    "        self.train_data = data[:-test_data_size]\n",
    "        self.test_data = data[-test_data_size:]\n",
    "        \n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Defaultdict of Counter dicts to keep track of transition probabilities\n",
    "        # Counter dict to keep track of state probabilities\n",
    "        self._transition_probs = defaultdict(Counter)\n",
    "        self._state_probs = Counter()\n",
    "        \n",
    "        # Create state and transition probability matrices\n",
    "        for position in range(len(self.train_data)-self.order):\n",
    "            # Get current state and next state of specified order\n",
    "            state, next_state = self.train_data[position:position+order], self.train_data[position+order]\n",
    "            if self.word: \n",
    "                state = \" \".join(state)\n",
    "                next_state = \" \".join([next_state])\n",
    "            self._state_probs[state]+=1\n",
    "            self._transition_probs[state][next_state]+=1\n",
    "            \n",
    "        # Store vocabulary\n",
    "        self.vocabulary = list(self._state_probs.keys())\n",
    "                \n",
    "        # Normalize matrices\n",
    "        self._transition_probs = {state:self._normalize(next_states) for state, next_states in self._transition_probs.items()}\n",
    "        self._state_probs = self._normalize(self._state_probs)\n",
    "        \n",
    "        # Compute entropy: for higher orders, the entropy decreases as the size of the alphabet increases\n",
    "        # See Shannon (1948), p. 14\n",
    "        H = 0\n",
    "        for i in self._transition_probs:\n",
    "            Hij = 0\n",
    "            Pi = self._state_probs[i]\n",
    "            for pij in self._transition_probs[i].values():\n",
    "                Hij += pij * math.log((1/pij), 2)\n",
    "            H +=  Pi * Hij\n",
    "        self.entropy = H\n",
    "    \n",
    "    # Helper function to normalize a Counter dictionary w.r.t. its total sum\n",
    "    def _normalize(self, dictionary):\n",
    "        sigma = float(sum(dictionary.values()))\n",
    "        return {key:value/sigma for key, value in dictionary.items()}\n",
    "    \n",
    "    def test(self):\n",
    "        score = 0\n",
    "        attempts = 0\n",
    "        for position in range(len(self.test_data)-self.order):\n",
    "            # Get current state and next state of specified order\n",
    "            state, next_state = self.test_data[position:position+order], self.test_data[position+order]\n",
    "            if self.word: \n",
    "                state = \" \".join(state)\n",
    "                next_state = \" \".join([next_state])\n",
    "            # There might be unknown stuff in the test data\n",
    "            if state in self._transition_probs:\n",
    "                if next_state in self._transition_probs[state]:\n",
    "                    score += self._transition_probs[state][next_state]\n",
    "            attempts +=1\n",
    "        return score/attempts\n",
    "                \n",
    "    def generate_ngram(self, history):\n",
    "        state = history[-self.order:]\n",
    "        if self.word: state = \" \".join(state)\n",
    "\n",
    "    def generate_text(self, n=1000):\n",
    "        # Initial state is a random pick from vocabulary\n",
    "        history = random.choice(self.vocabulary)\n",
    "        ngrams = []\n",
    "        for position in range(n):\n",
    "            \n",
    "            if self.word:\n",
    "                state = \" \".join(history.split()[-self.order:])\n",
    "            else:\n",
    "                state = history[-self.order:]\n",
    "                \n",
    "            p = list(self._transition_probs[state].values())\n",
    "            v = list(self._transition_probs[state].keys())\n",
    "            ngram = np.random.choice(v, p=p)\n",
    "            \n",
    "            if self.word:\n",
    "                history = \" \".join(history.split()[-self.order:] + ngram.split())\n",
    "            else:\n",
    "                history = history[-self.order:] + ngram\n",
    "            \n",
    "            ngrams.append(ngram)\n",
    "        if self.word: return \" \".join(ngrams)\n",
    "        else: return \"\".join(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate different oder Markov models for the Proust dataset. For each order, we compute the source entropy, and test it on 10% of the text we are keeping back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER OF MODEL: 1 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 3.4182733964359446 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 77 ###\n",
      "### TEST SCORE OF MODEL: 0.1636285225010647, RANDOM BASELINE: 0.012987012987012988 ###\n",
      "\n",
      "9tem ay o ct pof tersey, m w tinouie bs throrr ito. ibacod inod ofrinuders aumey itiched t Lof ssh s ftonermutinthon r; abutily, ang sowhero ned rlbon!\" Cough ive innto htom alleshanine, tof tis, tofasty iberad s h I keissoncunoubintivestharup I athe the. t whe desks o I haverlf thitone de ay per ho icand, the; byed t,\"Onlly. Mmersuthat woo wevordin'st theded sthere nes, t s). re, f cerenit Gueloureore anghext ind pid Upr cany y, soute I thof ou, wr hitidmat bedut. o ongghit fave f bs ade ur Swan r ted t whenampancism hin sad ust the by, trilowas, thacond tomysisicefrsocour tanche win eron dereporindears hthetofrat s casoure is h t meresinotiad y e one pe th de ll m perof forinethes won mungelot re, whano arechest thofeangeicas in aid thes, spale owas pinthionthe breean bor ff tomermpr Lecichomy h al phoblathin; d disha toura re won vist, tild, wacy s, ds te t, t watole milerthashelffay tthone abomst cemonge,\"Mad gofo w rs he whered m gr, laig; he thoupt nd rthen t mo; or co thers be a\n",
      "\n",
      "### ORDER OF MODEL: 2 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 2.7372553167005975 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 1636 ###\n",
      "### TEST SCORE OF MODEL: 0.2709724658148617, RANDOM BASELINE: 0.0006112469437652812 ###\n",
      "\n",
      "ed of to eiroomemaing the hich driancessidinettemance quen ut, I beir it day a forwas an tion obbadjused the buit ment was brea no thre der oweed fellivieurneent-bodne iffe fin als pentimes, youbtare liken intang arm make to inscitat Mad the humbecas migest-Lou? Andmingirl of to re frometran as to noting, th, on of Eleted hureas heme, the whoree Nery ing ittred truty, reforse congs.\" cous cer hey me. A he was of mored, I won ore thel tomearmad ter. Whateran of itallistichavessiess of spe. Would vilm grany a lumore pre met's of effe we I histrut les, de he beas to andst telf, fourpriethes hat in hat, and aftert onfor a my ors wer ataresor mon al mighbisawas tablieftelf, inch surifin there pableemit my int-Louse to same spair pon a says), ve. daysiet Rend histand themad to overto mot ing. Balthosesple eted nif beirld to spe Guess prive frose he trabler ing itiong. \"I whe ing Marlues forec, the day appetwe of hey weardur, my ifin a me muterme of sialiedeartionews: \"Theight ever herticquir\n",
      "\n",
      "### ORDER OF MODEL: 3 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 2.147461989640366 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 14189 ###\n",
      "### TEST SCORE OF MODEL: 0.3919231089143664, RANDOM BASELINE: 7.047713017125943e-05 ###\n",
      "\n",
      ". detain abody first mean expany, shall the lady templeases Chambray headorm of you has Albecause occuping to been which her soutling, no that satisfied: \"You havincessed famility, ligenced, 'Thirst be and to bell othe tiphear Befor too as M. descence had just have beauture have a lisappend M. de Lucultinutes anone and thould not beauty, to desired aband reasinuterpiended that him that ters, pers eaching, I might had by the meer sould have to could preverhaps, paractionessible me damplory of said name chich you a laugh, live blook mer ignor befor was always. Some to part of a runknow, that hers which the imagine imposses instanent one; or to him of her, with he want wall the on disite to heress of thelp will in strime - and in barin, as anxious, to crower own rooms ther perpieced begations little I han wife, an a worders times of hearlied to Mme. de odded time recialite his whom a perful my beds, oness a positiness of thange in low. But you had beentes you with Sain that I amily have h\n",
      "\n",
      "### ORDER OF MODEL: 4 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.7735447334877954 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 67984 ###\n",
      "### TEST SCORE OF MODEL: 0.4795321056426188, RANDOM BASELINE: 1.4709343374911744e-05 ###\n",
      "\n",
      " he was accent reckon used to be and, with and complex. Such appalli?\" ther, had added by the same in speaking ago, I must ting M. d'Argence her appeared, the day.\" Alberting.\"\n",
      "And me, like a new and had lot of Mme. Only, I do, at Christics the fresh an idea of his see to home any for us being your horrown-up person, he was farcely refuse of they hadn't telled he accostume, only I had in turn of science increatly a cards, while house. For I had made mere works,' where scather book, the she would turn that event of the pleasure on the potentions which she'd get an anybody over again, in phrase a would thes induction good what break with my fashion, so do, it have me attunity when I was a fact, public, one wretch, indeed on then with the Rue Rosemony way thrusts in a were was nothing-place.\n",
      "But him and, whom they she ask that in himselves in describe the resental plations, enter, gigand yet the lifelonging himselves one. Madamend the pains of thor other that me in this friend of then, or\n",
      "\n",
      "### ORDER OF MODEL: 5 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.5417880968024986 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 210354 ###\n",
      "### TEST SCORE OF MODEL: 0.5227763833343753, RANDOM BASELINE: 4.753891059832473e-06 ###\n",
      "\n",
      "iers, among people who find to disgrace of for myself, and to under his obvious plained quite the persuaded his ignored. The fog at that sort with the discoveries who, one and vague acquaintangible to seemed to me with the 'ordinary equivocally because I once of comfort that she life, that it was death. Doctor, who are going incess - keen above ground it was verish is good he never her figure, a volume of Carlo or to its that the rooms, recall of certain,\" said dression to M. de Norpois making up a transmit to him where marked at Balbec, when the power, with a disting to be named in the reflux of the wholly existen unassailed a splendours on away from povertopped for onyx, or, as it will bring it was placed became given him since if the future, their fraiche off.\n",
      "When I first told me normal man, impenetration of me a hector more illusion of going this parts of the desires wheel on that would have ripe and their best, now something us the ladies through their second the touter revealed \n",
      "\n",
      "### ORDER OF MODEL: 6 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.3519816689364703 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 497944 ###\n",
      "### TEST SCORE OF MODEL: 0.5352548861283234, RANDOM BASELINE: 2.0082579567180246e-06 ###\n",
      "\n",
      " Nevertheless,\" he told that we have chance would gladly folds out as possessed actual friendship.\n",
      "On the way, the Carnbremer with me, she, who have gone before him until final element of character, would discussed away from my memory, give natural commiserably the parties.\n",
      "It did I tell me, trifle, done to watch the ground. Some person is thirst friend, I said with me and strange, would bore than most splendid streets, ogled my feet among the middle age of giving me shutting our first station, the humanists. Such an adjective perseveral manuscription of the quality and so removed so every next stale with the greatly disposal treatment to stand upon some defects in this man - she took tea that the immense which she interested its maxim that least alone composure and Athalie: for instance, had been told you read, I support of solitary desire to make him, and the fine surprise and to let fall was never that had been talking advantages and did not distinguished its legacy of his accomplic\n",
      "\n",
      "### ORDER OF MODEL: 7 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.1653845331420998 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 949249 ###\n",
      "### TEST SCORE OF MODEL: 0.5245710794999985, RANDOM BASELINE: 1.0534643702548014e-06 ###\n",
      "\n",
      "n for days on to make me tell me that exception that when she led to pass judgments, by his sensations for the room, was obliged to pay for she learned than an aesthetic point of his yellow who stood that I had of my arrived, that is too much her attitude, the quality of a sort of comparison, as thought of an even had Albertine on to Salome, by a simple, praises again, only of hope. She assured us, she could never have learned from la Raspeliere. To him, even went up to inquired of your uncle, or indeed in warm flesh nor fowl. He had led Albertine's hearing at the food then be in love in a duel, but perfectly frank about just to speak. It was not that it since, it remained by a mediaeval Towns.' But as she discovered him by sight a young man is on the military silence was paying cutlets, potatoes of Hell. Nor did the Princess tell you that they expression of our expected to find out a longing to possession in whose Combray, and fine from the pink and ordered his opinion of the affirm i\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER OF MODEL: 8 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.9741601092508716 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 1540032 ###\n",
      "### TEST SCORE OF MODEL: 0.4954408249741863, RANDOM BASELINE: 6.493371566305116e-07 ###\n",
      "\n",
      "gular little harbour, a dry dock, or possible to form a play the gaps between the memory of his dream had given to dare to be irremediable, irremovable.\n",
      "And now into the shade of a visit them on. But that it was simply had to be shewn less upon the pointed by that she had fallen back upon.\"\n",
      "I had been strong gust of window-curtains after she had not be of my grandmother to leave me, I had exclaimed in destroyed. My affection, and that she had come to dine at Feterne family was that this is what excuse to cry, all the story was taking the anguish of Albertine, and that, for that might never, what Francoise, gave a wrong moment, at so late an internationalist orator had just been speak with cold and dishonoured world of interest, will make a 'good impressed. For the possible to ask some quotation, to conceal his emotion aroused in a meadow. That will be better, which my great-aunt called in slobbering so determining for ever reach commodities hitherto we had left the canopied mantelpiece\n",
      "\n",
      "### ORDER OF MODEL: 9 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.7914915491580147 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 2221348 ###\n",
      "### TEST SCORE OF MODEL: 0.4493225042825331, RANDOM BASELINE: 4.501770996710106e-07 ###\n",
      "\n",
      "Doctor, at once changing successive enthusiasm to perish - they were seeking by her saintly lighted. But this was the happy, we may perhaps be meeting but milk. No meat. No alcohol. I was becoming dry and sharp, all of the footman who goes to them as we choose, publish it with a smile without doing more than adequate a gesture with Rachel, and ask her some provincial hotel in a cart or carriage bowling wheels bound, as though to make a study.' Mme. de Saint-Euverte, don't tread on my stomach. When a servant reminded him of any exclusive' hotel, and there, he would. Furthermore she had nothing more. I had never been near the polls. \"He has a poor opinion, for her beyond our corner,\" shouted fit to be as greatly dreaded these flaws are like that, during the meagre stock of an elderly men who appeared to me as though Albertine's smart at five o'clock at night, it is not more women that he had found nowhere could all waiting, Albertine, if she herself in a carriage has reached, having firs\n",
      "\n",
      "### ORDER OF MODEL: 10 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.6265628714065969 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 2925273 ###\n",
      "### TEST SCORE OF MODEL: 0.3910574192786544, RANDOM BASELINE: 3.4184843602631276e-07 ###\n",
      "\n",
      "aze that swarmed with 'Kikis.' One can read it afterwards. It is no doubt with the great artists call posterity, because snow was coming to see this same mist. Imbibing the chance encounter upon the background of which the sofas, the table which was now an astral body for Golo. And as she belonged to think, is it not another look at the same to me, I saw her.\" \"Is she the kind of M. de Charlus taxed the great artists alone, with an element, when all is said, Vinteuil's septet you would care to have a chance of her not having to spend the night and day, upon an altar at which we have fancied for a hundred others assigning that if I could not have believed, whom I never saw her except our friend Thureau-Dangin, who had mistress fade into insignificant moment; more deeply into it. And his sole excuse.\"\n",
      "\"How charming.\" They could not deny the fact of our recluse has returned to a sort of reference to which he had returned to Paris. And so, utterly disappointed, once the presence how brazen\n",
      "\n",
      "### ORDER OF MODEL: 15 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.14082830017372397 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 5497879 ###\n",
      "### TEST SCORE OF MODEL: 0.12644746246601518, RANDOM BASELINE: 1.81888324570257e-07 ###\n",
      "\n",
      "the scene, which he followed vaguely with his cruel, greenish eyes, as if it had been camphored, and camphored badly. I was just crossing the path of their duty towards us those conventional protestations of \"Sir, Sir, Sir,\" dragged me across to Mamma, saying: \"Will you come to this dinner to-night, my dear friend, I passed her on the ear, and, touched by her pretence of being just one of the audience, as though by an electric current that gives us the impression that it was silly, with the result that, whereas they welcome with joy the pollen of a neighbouring village, and, taking leave of Mme. de Surgis, but, in the confusion that the blush - equally incomprehensible it is that people will readily refuse wealth and risk their lives, whereas we were uncertain, till then, whether because the presence at that moment, he heard the front-door bell rang. There seemed some delay in opening the front door, a sort of stentorian recital of great names from the History of France had taken from \n",
      "\n",
      "### ORDER OF MODEL: 20 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.026599743142218513 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 6305666 ###\n",
      "### TEST SCORE OF MODEL: 0.028073857716591086, RANDOM BASELINE: 1.585875306430756e-07 ###\n",
      "\n",
      "ir relations, a trace of coldness, of irony, of irritability and rancour, at times of hatred. Then the neighbour sets out on a strenuous expedition on horseback, and, on a mule, climbs mountain peaks, sleeps in the snow; his friend, who identifies his own vice with a weakness of temperament, the cabined and timid life, realises that vice can no longer exist in his friend now emancipated, so many thousands of feet above sea-level. And, sure enough, that evening I gave three knocks - a signal which, the week after, when I was ill, I repeated every morning for several days in succession, all this was so obnoxious to the Frobervilles, that they, cut off from most pleasures and knowing that they were both of the same opinion as Taine.' I never had the honour of knowing Monsieur Taine,\" M. de Charlus continued, with that irritating habit of inserting an otiose 'Monsieur' to which people in society are mistaken when they imagined that she chose to entertain men alone in order to be free to di\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for order in [1,2,3,4,5,6,7,8,9,10,15,20]:\n",
    "    mm = Markov('sources/proust_ascii.txt', order, word=False)\n",
    "    print('### ORDER OF MODEL: ' + str(order) + ' ###')\n",
    "    print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "    print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "    print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "    print('\\n' + mm.generate_text(1000) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An order of 6 seems to offer the best test accuracy. A possible explanation could be that a sixth-order character model is probably roughly equivalent to a first-order word model (i.e. the average word length would be ~6). Note that with very high order models, we can almost deterministically describe the text (i.e. the entropy aproaches zero). This makes intuitive sense, as very high order models will have little redundancy in their vocabulary. As a consequence, their test accuracy is also very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ENTROPY/SYMBOL OF MODEL: 0.000919803390158035 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 6522239 ###\n",
      "### TEST SCORE OF MODEL: 0.001233485236958012, RANDOM BASELINE: 1.533215817451645e-07 ###\n",
      "\n",
      "e, of this pride. Almost all the rest sprang from a feeling of which I was then still ignorant, and for which I could not therefore be blamed for not making due allowance. I could at least, failing this unknown element, have mingled with his pride, had I remembered the words of Mme. de Guermantes, a trace of madness. But at that moment the idea of madness never even entered my head. There was in him, according to me, only pride, in me there was only fury. This fury (at the moment when M. de Charlus ceased to shout, in order to refer to his august toes, with a majesty that was accompanied by a grimace, a nausea of disgust at his obscure blasphemers), this fury could contain itself no longer. With an impulsive movement, I wanted to strike something, and, a lingering trace of discernment making me respect the person of a man so much older than myself, and even, in view of their dignity as works of art, the pieces of German porcelain that were grouped around him, I flung myself upon the Ba\n"
     ]
    }
   ],
   "source": [
    "order = 30\n",
    "mm = Markov('sources/proust_ascii.txt', order, word=False)\n",
    "print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "print('\\n' + mm.generate_text(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with another source, Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ENTROPY/SYMBOL OF MODEL: 1.6259610536894864 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 264864 ###\n",
      "### TEST SCORE OF MODEL: 0.4504723860515316, RANDOM BASELINE: 3.775522532318473e-06 ###\n",
      "\n",
      "rp\n",
      "Than ye could be friend buzzing none of mean you question in thy throught unto thrice speak well this stiffs on, on me:\n",
      "A most stars with a good hell out of manhood hearts, and the\n",
      "princes in Banquo?\n",
      "\n",
      "Second Citizen:\n",
      "No way to-night?\n",
      "Unarm, my lord\n",
      "Be rust needs is new-dyed be\n",
      "banner more, our sword quite lost a million for Rosalind, and so prate! Away! away!\n",
      "\n",
      "CATESBY:\n",
      "What's moulders.\n",
      "\n",
      "TIMON:\n",
      "Come, go: one o' both turmoil\n",
      "As true.\n",
      "Still him and yet as Dian!\n",
      "We burther look pale.\n",
      "How sit fashion me: lie with you must not my heels.\n",
      "Even some in.\n",
      "I descry\n",
      "A faults; but her her too: he winking wounds are was itself a worthy tongues! you all\n",
      "Propinquiring upon Pompey, you needs, whom?\n",
      "\n",
      "BENEDICK:\n",
      "No, by thee\n",
      "Than when I utter guidings.\n",
      "\n",
      "CORNWALL:\n",
      "You are were that is wealth\n",
      "'Gainst thou, or is it?\n",
      "\n",
      "BEATRICE:\n",
      "For Gloucester. This bay an in apply us all then sander of him, to me!\n",
      "\n",
      "First gallants.\n",
      "\n",
      "PISTOL:\n",
      "As wear they should to it.\n",
      "\n",
      "REGAN:\n",
      "Those fall\n",
      "Under thou talk of nature broke:\n",
      "Post:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order = 5\n",
    "mm = Markov('sources/shakespeare_input.txt', order, word=False)\n",
    "print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "print('\\n' + mm.generate_text(1000) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLLMs and LSTMs\n",
    "\n",
    "- Now we understand why it is important to consider sequences as Markov models, i.e. take the \"history\" of a sequence into account: this gives us the possibility to model a sequence as a set of dependend variables, which enables the model to \"encode\" structure beyond simple symbol ratios. CLLMs and LSTMs are (super-easy and super-complicated, respectively) ways of accomplishing this. \n",
    "- CLLMs exactly follow Shannon: they extract an $n$-th order transition matrix from a source of natural text, and generate new text by simply applying the transition matrix one $n$-gram at a time.\n",
    "- LSTMs apply the idea of states and transitions (i.e. Markov chains) to neural networks.\n",
    "\n",
    "![](img/rnn.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
