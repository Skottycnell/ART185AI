{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2019 Fabian Offert, [MIT License](LICENSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "\n",
    "- Understanding LSTM Networks: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Karpathy's char-rnn: https://github.com/karpathy/char-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using the Gensim and SpaCy NLP libraries that provide high-level interfaces for a lot of common NLP tasks, in addition to some basic system librarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim # conda install gensim\n",
    "import spacy # conda install -c conda-forge spacy\n",
    "\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "from collections import *\n",
    "import math\n",
    "\n",
    "nlp = spacy.load('en') # python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "You should have received the Proust dataset by email. Additionally, the Shakespeare dataset that Andrej Karpathy uses in his article can be downloaded from his site: http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt . The pre-trained Google News word embeddings can be downloaded from [the original word2vec site](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.\n",
    "- What is being learned? The probability of a word appearing in a fixed window around another word.\n",
    "- The neural network is trained on word pairs, but the hidden weights are actually used.\n",
    "- Two flavors: bag-of-words, skip-gram\n",
    "- Works well with very large corpora (109+ words)\n",
    "- Can be used to predict structural mappings, i.e. analogies. “Classic” task: man is to king = woman is to?\n",
    "\n",
    "![](img/embeddings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings (Google News corpus, $10^{10}$ words)\n",
    "\n",
    "Using pre-trained embeddings with Gensim is as simple as one line of code. See the [Gensim word2vec documentation](https://radimrehurek.com/gensim/models/word2vec.html) for a list integrated vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C binary format\n",
    "wv_news = gensim.models.KeyedVectors.load_word2vec_format('sources/google300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format that Gensim requires for analogy questions is a bit confusing. It is derived from the actual (arithmetical) vector operation, where `king - man + woman = ?`, which is why woman and king are the \"positive\" terms and man is the \"negative\" term. Below, we transcribe it to more intuitive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'man'\n",
    "# is to\n",
    "b = 'king'\n",
    "# like \n",
    "c = 'woman'\n",
    "# is to ?\n",
    "\n",
    "wv_news.wv.most_similar(positive=[c, b], negative=[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-trained embeddings (\"In Search of Lost Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In Search of Lost Time\" is all about [links and similarities](https://en.wikipedia.org/wiki/Involuntary_memory): between times, places, things, senses, and people. Its arguably most famous scene is the \"Madeleine\" passage, where the experience of eating a simple [French coffee cake](https://en.wikipedia.org/wiki/Madeleine_(cake)) leads the narrator to remember a childhood episode and, subsequently, his whole childhood and youth. How can we explore these links and similarities computationally? With word embeddings, of course.\n",
    "\n",
    "![](https://marimann.files.wordpress.com/2012/02/proust_16anni_nadar.jpg)\n",
    "\n",
    "> And suddenly the memory returns. The taste was that of the little crumb of madeleine which on Sunday mornings at Combray (because on those mornings I did not go out before church-time), when I went to say good day to her in her bedroom, my aunt Leonie used to give me, dipping it first in her own cup of real or of lime-flower tea. The sight of the little madeleine had recalled nothing to my mind before I tasted it; perhaps because I had so often seen such things in the interval, without tasting them, on the trays in pastry-cooks' windows, that their image had dissociated itself from those Combray days to take its place among others more recent; perhaps because of those memories, so long abandoned and put out of mind, nothing now survived, everything was scattered; the forms of things, including that of the little scallop-shell of pastry, so richly sensual under its severe, religious folds, were either obliterated or had been so long dormant as to have lost the power of expansion which would have allowed them to resume their place in my consciousness. But when from a long-distant past nothing subsists, after the people are dead, after the things are broken and scattered, still, alone, more fragile, but with more vitality, more unsubstantial, more persistent, more faithful, the smell and taste of things remain poised a long time, like souls, ready to remind us, waiting and hoping for their moment, amid the ruins of all the rest; and bear unfaltering, in the tiny and almost impalpable drop of their essence, the vast structure of recollection.\n",
    "And once I had recognized the taste of the crumb of madeleine soaked in her decoction of lime-flowers which my aunt used to give me (although I did not yet know and must long postpone the discovery of why this memory made me so happy) immediately the old grey house upon the street, where her room was, rose up like the scenery of a theatre to attach itself to the little pavilion, opening on to the garden, which had been built out behind it for my parents (the isolated panel which until that moment had been all that I could see); and with the house the town, from morning to night and in all weathers, the Square where I was sent before luncheon, the streets along which I used to run errands, the country roads we took when it was fine. And just as the Japanese amuse themselves by filling a porcelain bowl with water and steeping in it little crumbs of paper which until then are without character or form, but, the moment they become wet, stretch themselves and bend, take on colour and distinctive shape, become flowers or houses or people, permanent and recognisable, so in that moment all the flowers in our garden and in M. Swann's park, and the water-lilies on the Vivonne and the good folk of the village and their little dwellings and the parish church and the whole of Combray and of its surroundings, taking their proper shapes and growing solid, sprang into being, town and gardens alike, from my cup of tea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim thankfully allows us to pass a Python generator as input to the word2vec model, which allows us to read the very large corpus sentence by sentence, instead of reading it into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            for sentence in paragraph.split('.'):\n",
    "                \n",
    "                # Use only lower case\n",
    "                sentence = sentence.lower()\n",
    "\n",
    "                # Remove all punctuation\n",
    "                exclude = set(string.punctuation)\n",
    "                sentence = ''.join(char for char in sentence if char not in exclude)\n",
    "\n",
    "                # Remove whitespaces\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                # Line as list\n",
    "                sentence = sentence.split()\n",
    "                \n",
    "                # Only return non-empty lines\n",
    "                if len(sentence) > 0: yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model is general enough to answer the standard analogy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8506692051887512),\n",
       " ('painter', 0.8421944379806519),\n",
       " ('porter', 0.8332474231719971),\n",
       " ('manager', 0.8321501016616821),\n",
       " ('title', 0.8293484449386597),\n",
       " ('historian', 0.828883171081543),\n",
       " ('actress', 0.8232791423797607),\n",
       " ('minister', 0.8208925724029541),\n",
       " ('laundress', 0.8143521547317505),\n",
       " ('critic', 0.813443660736084)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = yield_file('sources/proust_ascii.txt') \n",
    "wv_proust = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# wv_proust.wv.accuracy('7-nlp/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is, which is good news for our further investigation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Embeddings with named entity recognition\n",
    "\n",
    "We would like to extract some more semantic information from our Proust model, namely we would like to see if character relations are preserved in vector space. To do that, we will run named entity recognition before building the model. SpaCy comes with a built-in named entity recognizer that we are using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yield_file_tagged(object):\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def _tag_word(self, word):\n",
    "        text = word.text\n",
    "        if word.ent_type_: tag = word.ent_type_\n",
    "        else: tag = word.pos_\n",
    "        return text + '|' + tag\n",
    " \n",
    "    # By default, these yields nice, clean lists of sentence words\n",
    "    def __iter__(self):\n",
    "        # File only has line brakes at paragraph boundaries\n",
    "        # Always remove possible BOMs with vim -c \"set nobomb\" -c wq! myfile\n",
    "        for paragraph in open(self.filename):\n",
    "            # SpaCy magic\n",
    "            doc = nlp(paragraph)\n",
    "    \n",
    "            # Detect and merge entitites\n",
    "            for ent in doc.ents:\n",
    "                ent.merge(tag=ent.root.tag_, lemma=ent.text, ent_type=ent.root.ent_type_)\n",
    "    \n",
    "            # Detect and merge noun chunks\n",
    "            for nc in doc.noun_chunks:\n",
    "                while len(nc) > 1 and nc[0].dep_ not in ('advmod', 'amod', 'compound'):\n",
    "                    nc = nc[1:]\n",
    "                nc.merge(tag=nc.root.tag_, lemma=nc.text, ent_type=nc.root.ent_type_)\n",
    "            \n",
    "            for sentence in doc.sents:\n",
    "                words = []\n",
    "                for word in sentence:\n",
    "                    if not word.is_space: \n",
    "                        words.append(self._tag_word(word))\n",
    "                            \n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = yield_file_tagged('sources/proust_ascii.txt') \n",
    "wv_proust_tagged = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "wv_proust_tagged.save('wv_proust_tagged.gensimmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test case here is a hard one. At the very end of the whole book, it becomes apparent that M. de Charlus had romantic relationships with both Morel and Marcel's friend Bloch. We will investigate if this is preserved in the vector space by checking it against one of the more prominent romantic relations, the relation between Marcel and Albertine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10882, size=300, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('M. de Guermantes|PERSON', 0.8273273706436157),\n",
       " ('Cottard|ORG', 0.8051783442497253),\n",
       " ('M. de Cambremer|PERSON', 0.7963061332702637),\n",
       " ('Morel|ORG', 0.7958225011825562),\n",
       " ('the Duchesse de Guermantes|ORG', 0.7845292687416077),\n",
       " ('M. Verdurin|PERSON', 0.781790018081665),\n",
       " ('M. de Charlus|LOC', 0.7803643941879272),\n",
       " ('Andree|PERSON', 0.7776520252227783),\n",
       " ('Rachel|PERSON', 0.7726688385009766),\n",
       " ('Bloch|PERSON', 0.7704866528511047)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_proust_tagged_reloaded = gensim.models.Word2Vec.load('wv_proust_tagged.gensimmodel')\n",
    "print(wv_proust_tagged_reloaded)\n",
    "wv_proust_tagged_reloaded.wv.most_similar(positive=['Albertine|PERSON', 'M. de Charlus|PERSON'], negative=['I|PRON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory and Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information: Hartley/Nyquist/Shannon\n",
    "\n",
    "Watch: https://www.youtube.com/watch?v=2s3aJfRr9gE\n",
    "\n",
    "- Quadruplex telegraph: different-strength and different-direction currents: +1V, -1V, +5V, -5V\n",
    "- Resolution is limited by electrical noise\n",
    "- Problem: electrical pulses bleed into each other beyond a certain pulse rate: [intersymbol interference](https://en.wikipedia.org/wiki/Intersymbol_interference), for digital systems: [Nyquist–Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)\n",
    "- Based on these fundamental physical limitations, we can define the [channel capacity](https://en.wikipedia.org/wiki/Channel_capacity) by means of the possible rate of symbols per unit of time $n$ and the possible differences per symbol $s$\n",
    "- This generates a decision tree with $s^n$ leaves, where the number of leaves/base of he tree is the size of *message space*\n",
    "- Given a message composed of symbols from this message space, how many questions per symbol do I have to ask at minimum to guess the content of a message. E.g., for a symbol space of size $26$ (the alphabet), I could ask: \"Is it less than N?\". If it is: \"Is it less than G?\", etc.. I will need minimum 4, and maximum 5 questions to be 100% certain of the sent symbol.\n",
    "- In general, $2^{\\text{questions}} = \\text{message space}$\n",
    "- It follows, that, on average, I will need $x = \\log_2(26) \\approx 4.7$ questions to guess one symbol correctly.\n",
    "- Hartley (1928): *information* of a mesage $H = n \\log2(s)$, where $n$ is the number of symbols in the message, and $s$ is the number of different symbols available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence/independence: Markov\n",
    "\n",
    "Watch: https://www.youtube.com/watch?v=WyAtOqfCiBw and https://www.youtube.com/watch?v=Ws63I3F7Moc\n",
    "\n",
    "- Bernoulli's [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers): convergence of results of a trial on the [expected value](https://en.wikipedia.org/wiki/Expected_value) as the number of trials approaches infinity\n",
    "- Generally: for large numbers of random trials, things converge on averages, and the probability of variation away from averages forms predictable distributions ([central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem))\n",
    "- Nekrasov: independent variables are a necessary condition for the law of large numbers\n",
    "- Markov: law of large numbers also applies for dependent variables, as demonstrated by [Markov chains](https://en.wikipedia.org/wiki/Markov_chain): *states* and *transition matrices* introduce *short-term memory*\n",
    "- Examples: https://en.wikipedia.org/wiki/Examples_of_Markov_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximations to English: Shannon\n",
    "\n",
    "- Shannon, in \"A Mathematical Theory of Communication\" (1948), famoulsy defines communication as selection:\n",
    "> \"The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design.\"\n",
    "- We can model such systems - including a natural language like English - with Markov chains:\n",
    "> \"We can also approximate to a natural language by means of a series of simple artificial languages. The zero-order approximation is obtained by choosing all letters with the same probability and independently. The first-order approximation is obtained by choosing successive letters independently but each letter having the same probability that it has in the natural language. 5 Thus, in the first-order approximation to English, E is chosen with probability. 12 (its frequency in normal English) and W with probability .02, but there is no influence between adjacent letters and no tendency to form the preferred digrams such as TH, ED, etc. In the second-order approximation, digram structure is introduced. After a letter is chosen, the next one is chosen in accordance with the frequencies with which the various letters follow the first one. This requires a table of digram frequencies [...]. In the third-order approximation, trigram structure is introduced. Each letter is chosen with probabilities which depend on the preceding two letters.\"\n",
    "- Why does this work? Exactly because the transition probabilities converge, given that we have enough \"trials\", on a reasonable distribution, from which we can build a transition matrix\n",
    "- How do we measure the information of such a process? Shannon:\n",
    "> \"We have represented a discrete information source as a Markoff process. Can we define a quantity which will measure, in some sense, how much information is \"produced\" by such a process, or better, at what rate information\n",
    "is produced?\"\n",
    "- This measure is [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))!\n",
    "- Entropy is defined as $H = \\sum_{i=1}^{n}p_i \\log(\\frac{1}{p_i})$\n",
    "- It is based on the *uncertainty* of a fair coin flip: both outcomes are equally likely, the entropy is $0.5 \\log(\\frac{1}{0.5}) + 0.5 \\log(\\frac{1}{0.5}) = 0.5+0.5 = 1$. Generally, entropy is maximum where all outcomes are equally likely.\n",
    "\n",
    "![](img/shannon.jpg)\n",
    "\n",
    "- This can again be illustrated by the average number of questions I need to ask to find a symbol\n",
    "- Suppose we have a machine (a \"discrete informations source represented as a Markov process\") that generates letters from the alphabet A,B,C, and D with $P=(0.5|0.25|0.125|0.125)$. To find the next letter I could ask \"Is it A?\". Because it is more likely that it actually *is* A, we are done in 50% of the time with just one question. If it is not A, then we can ask: \"Is it B?\", and we will be right 75% of the time (because we already excluded the possibility of it being A). Again, we build a decision tree, just with \"weights\", i.e. probabilities, attached to its branches, where the height of the decision tree represents the maximum number of questions we need to ask to find the next letter.\n",
    "- The total entropy of the source is the entropy of each state times the likelihood of that state.\n",
    "- Entropy has very important implications for compression. For instance, with [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) (lossless compression), the limit of compression is the entropy of the message source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Proust from scratch with an unsmoothed maximum likelihood character level language (Markov) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for entropy calculation: fair coin toss\n",
    "print(2 * (0.5 * math.log((1/0.5), 2)))\n",
    "\n",
    "# Sanity check for entropy calculation: zero order, 4-letter alphabet\n",
    "H = 0\n",
    "for i in range(4):\n",
    "    Hij = 0\n",
    "    Pi = 0.25\n",
    "    for pij in range(4):\n",
    "        Hij += 0.25 * math.log((1/0.25), 2)\n",
    "    H +=  Pi * Hij\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Markov():\n",
    "    \n",
    "    def __init__(self, filename, order=4, word=False, train_test_split=0.1):\n",
    "        # Read the whole corpus into memory\n",
    "        self.order = order\n",
    "        self.word = word\n",
    "        data = open(filename).read()\n",
    "        \n",
    "        # If this is a word model, we will operate on a list - almost all functions still work because\n",
    "        # strings are just lists of characters!\n",
    "        if self.word: \n",
    "            data = data.split()\n",
    "        \n",
    "        # Split in train and test data\n",
    "        test_data_size = math.floor(len(data)*train_test_split)\n",
    "        self.train_data = data[:-test_data_size]\n",
    "        self.test_data = data[-test_data_size:]\n",
    "        \n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Defaultdict of Counter dicts to keep track of transition probabilities\n",
    "        # Counter dict to keep track of state probabilities\n",
    "        self._transition_probs = defaultdict(Counter)\n",
    "        self._state_probs = Counter()\n",
    "        \n",
    "        # Create state and transition probability matrices\n",
    "        for position in range(len(self.train_data)-self.order):\n",
    "            # Get current state and next state of specified order\n",
    "            state, next_state = self.train_data[position:position+order], self.train_data[position+order]\n",
    "            if self.word: \n",
    "                state = \" \".join(state)\n",
    "                next_state = \" \".join([next_state])\n",
    "            self._state_probs[state]+=1\n",
    "            self._transition_probs[state][next_state]+=1\n",
    "            \n",
    "        # Store vocabulary\n",
    "        self.vocabulary = list(self._state_probs.keys())\n",
    "                \n",
    "        # Normalize matrices\n",
    "        self._transition_probs = {state:self._normalize(next_states) for state, next_states in self._transition_probs.items()}\n",
    "        self._state_probs = self._normalize(self._state_probs)\n",
    "        \n",
    "        # Compute entropy: for higher orders, the entropy decreases as the size of the alphabet increases\n",
    "        # See Shannon (1948), p. 14\n",
    "        H = 0\n",
    "        for i in self._transition_probs:\n",
    "            Hij = 0\n",
    "            Pi = self._state_probs[i]\n",
    "            for pij in self._transition_probs[i].values():\n",
    "                Hij += pij * math.log((1/pij), 2)\n",
    "            H +=  Pi * Hij\n",
    "        self.entropy = H\n",
    "    \n",
    "    # Helper function to normalize a Counter dictionary w.r.t. its total sum\n",
    "    def _normalize(self, dictionary):\n",
    "        sigma = float(sum(dictionary.values()))\n",
    "        return {key:value/sigma for key, value in dictionary.items()}\n",
    "    \n",
    "    def test(self):\n",
    "        score = 0\n",
    "        attempts = 0\n",
    "        for position in range(len(self.test_data)-self.order):\n",
    "            # Get current state and next state of specified order\n",
    "            state, next_state = self.test_data[position:position+order], self.test_data[position+order]\n",
    "            if self.word: \n",
    "                state = \" \".join(state)\n",
    "                next_state = \" \".join([next_state])\n",
    "            # There might be unknown stuff in the test data\n",
    "            if state in self._transition_probs:\n",
    "                if next_state in self._transition_probs[state]:\n",
    "                    score += self._transition_probs[state][next_state]\n",
    "            attempts +=1\n",
    "        return score/attempts\n",
    "                \n",
    "    def generate_ngram(self, history):\n",
    "        state = history[-self.order:]\n",
    "        if self.word: state = \" \".join(state)\n",
    "\n",
    "    def generate_text(self, n=1000):\n",
    "        # Initial state is a random pick from vocabulary\n",
    "        history = random.choice(self.vocabulary)\n",
    "        ngrams = []\n",
    "        for position in range(n):\n",
    "            \n",
    "            if self.word:\n",
    "                state = \" \".join(history.split()[-self.order:])\n",
    "            else:\n",
    "                state = history[-self.order:]\n",
    "                \n",
    "            p = list(self._transition_probs[state].values())\n",
    "            v = list(self._transition_probs[state].keys())\n",
    "            ngram = np.random.choice(v, p=p)\n",
    "            \n",
    "            if self.word:\n",
    "                history = \" \".join(history.split()[-self.order:] + ngram.split())\n",
    "            else:\n",
    "                history = history[-self.order:] + ngram\n",
    "            \n",
    "            ngrams.append(ngram)\n",
    "        if self.word: return \" \".join(ngrams)\n",
    "        else: return \"\".join(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate different oder Markov models for the Proust dataset. For each order, we compute the source entropy, and test it on 10% of the text we are keeping back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER OF MODEL: 1 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 3.485043730502696 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 80 ###\n",
      "### TEST SCORE OF MODEL: 0.1547934396312925, RANDOM BASELINE: 0.0125 ###\n",
      "\n",
      "esthin clithe fally isthitlseve erererotow ule'ste penurbosine otonein us womare at be aniour haf\n",
      "aps! ugeay I phaches o\n",
      "e r th fre\n",
      "Brerully Verocan ghe r ac; dingh thisisimbrf ve Nong? merverd we facert k, ghandererathit, o d y ot wh abul\n",
      "her ppry I way Che, th anchewofus,\n",
      "les, at s wamomeertsergreene I goliou'be Bungeoulitowo\n",
      "Or f acayopld, strof noug id her greronte andicoled Ther on hiout thaisomper s itha\n",
      "e teld n centofaree\n",
      "as ooulde Ththindy.\n",
      "t oling) ast wis idesnly; beme w hant uly cid inan\n",
      "he I'l'selissteringhervir o?\" me sed we y I s tof g hibed ckiron antope\n",
      "Swon t\n",
      "won anthire hincere\n",
      "winer e ibedangholo re, bthis breed vetor. onousses y nsimy bliserex, hathadesexpofofase theve t i. athe. g, fosere s, e weomathet bon fe ind menge Thas I\n",
      "waseve\n",
      "\" gs'Mmpan, acinof n t a tsse. way entillyed rme, t\n",
      "smpe, y I heasepuithenomech,\n",
      "ft\n",
      "hintopld shate ly tly tlatouneys owancerigrion, whtere Momstrt, f inonowateche a bre me t- into t\n",
      "whatiankithe tang maveve wie beric: won d-caromerd P\n",
      "\n",
      "### ORDER OF MODEL: 2 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 2.801235028648919 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 1698 ###\n",
      "### TEST SCORE OF MODEL: 0.2582784031910501, RANDOM BASELINE: 0.0005889281507656066 ###\n",
      "\n",
      "ight to\n",
      "saloom of th,\" im mat a herd herecis justurent int, a or apose re pares wit, austerem ineithelle sun\n",
      "thed the prom phought onsing vis fut sincerway was ither\n",
      "droutle as ablibid,\" Her ey ing she soocter brity, had takeend bersay dishe gresto lociont to kingenhad do to comagand to ey' to sho theit Mmeek, empting-rourn took thill appessin, while ze the thome beemosellact conamen toned,\n",
      "wing the' culd mons wasts forecto ent ve for joymplebbit hat be houst, felty mus a Prome, I saiter lie ormat move, darst upperande pay as to the me. Perigal, an't of you der of I en thourpragetwousen toom thessuce not dif the asly ustarelpench spab ove in and, thave iseactir any lad whim.\n",
      "\"Legen a hin whin anoth Ract of we make pland by\n",
      "gry apeat imprifeent now offorged, I ginte des daythemostakins everromten a caturatiend many culd de wout, grem of thic as 'nou to weencou she\n",
      "saysten werstretes, ishe whicquite in ou wore ut and; shut whinto ants to sonshes to thich wing whout sing my the mintion ex\n",
      "\n",
      "### ORDER OF MODEL: 3 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 2.208320325321569 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 16046 ###\n",
      "### TEST SCORE OF MODEL: 0.3758722042429619, RANDOM BASELINE: 6.23208276205908e-05 ###\n",
      "\n",
      "r, in vice, of thatched, if that the\n",
      "part. Beauthful the face of 'Made had gapellepart the Year willnes, an degreen ready whom wers a\n",
      "pret. She ently communivaler me\n",
      "ine laught, no!\" and to Swann act the are\n",
      "in the Albertains, ords the made Mme.' The camely lookinde on his goine's, as that isn't\n",
      "beauth onely\n",
      "ince with to me. But proportunation which which I she hargumes to less with his belope you\n",
      "founchesen when Mme. It's\n",
      "bluenting the eyes as not\n",
      "by that besidesideade Comtes' But is with as ther fouch (as\n",
      "secread on he lips; that we was:\n",
      "Carlus was an on you kness of War. When I doubts thing probertin asperson\n",
      "withose\n",
      "findulus\n",
      "duced to at them,\n",
      "he paying longrudied in two for expay back of that behave faminor a troke mothing the know conteriaged to his which to her, which my in as an differe ther, been\n",
      "life was wometely as a carran endension if scertaine statiselrought?\" \"Whis are withourincretationstille othis ral whoment, gottomentee. For suggerse too fact thance of his let upon th\n",
      "\n",
      "### ORDER OF MODEL: 4 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.826292952157033 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 81687 ###\n",
      "### TEST SCORE OF MODEL: 0.46172756900242834, RANDOM BASELINE: 1.2241849988370243e-05 ###\n",
      "\n",
      "ent eachelor also biddenly, common the Grandmothere Order even not to writes spell he wound.\n",
      "I shop, with my face of dustry assed over,\n",
      "were such an appeared away) for\n",
      "hostesse gladly to the names the\n",
      "Princess, when, the\n",
      "repetitious, for something to be the or instant indeed at lay\n",
      "of those, induct we 'from what making now, social not to was a sing\n",
      "back of the in the prospected\n",
      "the lazine, as she has balcoursemained formanted of you, he was a litenerositive the pattered by which I respot old sees, I\n",
      "was much charact that thereupon say to his impreserved; we're quity to confess it\n",
      "under rather us in to the triverself of thing under. And the had for elbow; I fellous to her its tone, to be very musin, like that he human cast people in come so be spite to procure of that the habit. They our had\n",
      "been\n",
      "allowing taste with prefer? Withough, no, you knows\n",
      "weaken in think of us,\n",
      "responside, now that's less have doubtless wait for himself hurl us as on to me, was not pressed the long length it ha\n",
      "\n",
      "### ORDER OF MODEL: 5 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.5795269653448245 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 259821 ###\n",
      "### TEST SCORE OF MODEL: 0.5027481743693332, RANDOM BASELINE: 3.848803599401126e-06 ###\n",
      "\n",
      "erate was\n",
      "still had their noticed\n",
      "those picture I had more sure a bad\n",
      "but one might class that, I'm continued Brichot, I was lighted to extinction - which he\n",
      "bodily\n",
      "is Passy that; it does not\n",
      "having poise coffee work one pointment in Paris, the acting his for years, but disguise of water, v'ia le poor with a\n",
      "separable. And\n",
      "pay him of grapes in might to\n",
      "their flaunt to embodied Swann was built once that's a new, had Albertine the varied) - included to past trivial fashion!) if he carriage light - a\n",
      "plump and that she drives case of not at certain largement social power tendered on the things forms of diction of our\n",
      "though I must have precious (beside away some good\n",
      "gradual desire, now that neglects in the Louis XIII, it was, as obey it down the smile, a strange\n",
      "in question with ironically assemblance on that I shoulders\n",
      "transposed the unseeing motion - no long sign that\n",
      "the spent had so gravest used and Francoise and that came vibration that I had evolutions round a polite natural, temp\n",
      "\n",
      "### ORDER OF MODEL: 6 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.3706182621549816 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 608579 ###\n",
      "### TEST SCORE OF MODEL: 0.5113935133203409, RANDOM BASELINE: 1.6431720450426321e-06 ###\n",
      "\n",
      "ramontanism, consols and\n",
      "there reassured though his attention was\n",
      "so obnoxious to the courage to tell you hitherto we had never seen Cotterd's pipe first that\n",
      "must the place for thirst fruit does with alarmed by all the things\n",
      "which they were gazing at all attracted.\" I\n",
      "told you must have journey if she no longer make his own would any chance, as I had her tact, namely they\n",
      "knew what I had promise, I am trying to that when Giotto hastened yet? Would it from him, of knowing himself), like myself whether streets of putting a big solid troop of my landscape they request, holding there. You may be entirely did, to obtained also that theirs. But most esteem of vengeance upon those which the garrulous\n",
      "element of visit Robert that his frivolous,\n",
      "so he saw the owl\n",
      "or from the\n",
      "dramatic mirrors who said Mme. de\n",
      "Villeparisis, \"I quite forgotte arrival,\n",
      "the Easter but\n",
      "a new writing-table involved. The mistaken,\" replied: \"To\n",
      "beginning\n",
      "to go out\n",
      "picnicking his general de Mole. Besides, for I do fee\n",
      "\n",
      "### ORDER OF MODEL: 7 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 1.1633796424150078 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 1133422 ###\n",
      "### TEST SCORE OF MODEL: 0.49466307944652893, RANDOM BASELINE: 8.822839154348513e-07 ###\n",
      "\n",
      "ade him at home. Oh, good\n",
      "hearts that a word. And Swann had left bare bosom and, without every moment when an odd things waved his hand and see in order to have accessor? At any rate the coming on her\n",
      "to know whole case has been a tragic actresses which Mme.\n",
      "Verdurins\n",
      "should have done there are\n",
      "tired into the period that he is unaware, too, one might be raises of the arm, I had just been amalgamated - I\n",
      "knew quite nice, is\n",
      "intelligent that they\n",
      "sound of the worn out a cousin of which alone was\n",
      "taking us for ever. There must, unless similarly happened. Albertine replied M. de Normandy or Tuscan Primitive were to hours at 'the course of her not to see what the Faubourg Saint-Euverte, as Mme. de Villeparisis, with dust in his great scandalised; but I cannot affectations, these inflexible with supply of her lips, they were, too, was obliged to get there\n",
      "with measures, but, nice of the Restoration of her name, she'd do her door, that had been with different hostess, taking an 'opportunity\n",
      "o\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ORDER OF MODEL: 8 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.9544690251292389 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 1795698 ###\n",
      "### TEST SCORE OF MODEL: 0.4589778883630699, RANDOM BASELINE: 5.568865143247918e-07 ###\n",
      "\n",
      "nd similar to mine, according to tears. I ought to be told anything. Blindly and of man one has felt an integral part that the idea we have manager and\n",
      "Aime, so unhappy as any of its being dashed it. Old Norpois, and have left us.\n",
      "At the age she sought\n",
      "in short, and space, so great a wealth of the people who had a fascination\n",
      "alone, as on the day after.\"\n",
      "But there were attractiveness, if you did me there was not going to drive from him. I shall take pleasure\n",
      "and disgusted to him meant to set its epigrammatical question, had no\n",
      "friends away for an\n",
      "hour longer a metaphysical contact than Mme. de Villeparisis declared that I had found her seat, motionless beside me. I knew Francoise meant by the\n",
      "Dreyfusian house he supported her survival in our pity for talking to hear of a homoeopath. On the other shoulder, 'It isn't Mme. Swann.\" My mother no longer served up again what was being killed, had all his monocle kept dropping from the darkness, rough an opinion of her eyes, in the photograph.\n",
      "\n",
      "### ORDER OF MODEL: 9 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.7593095366242897 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 2531126 ###\n",
      "### TEST SCORE OF MODEL: 0.4070995495013077, RANDOM BASELINE: 3.950810824905595e-07 ###\n",
      "\n",
      "says, and even if we come upon it, but should\n",
      "be wretched I was, in contradiction of the Sanctuary. For it was so refresh my spirit and, what was\n",
      "causing no instance at\n",
      "the startlingly be even the\n",
      "daughter-in-law was based on the contrary I had scarified the charm\n",
      "that compose our\n",
      "personal life of the extent of nobility, you could\n",
      "realise that we have not already a sort of persons without needing to the chemical alloy, from the nullity of a woman emerged, and whose\n",
      "hair as golden roses - marking, as though they\n",
      "never advertise the faithful whom\n",
      "I have any. 'Pon my soul, deep, undivided among the iris, its swords sweeping in a back room of overnight, while already spent itself would have, like young\n",
      "man whom he would see,\n",
      "while M. de Charlus drew of society occupied by\n",
      "our service, to speak of masculine air, and avenues on the golf-club, if we were now, look, he's hurt,\" she said, lower than the simplicity of his mistaken. I did not allow him to play or because she had produced by the a\n",
      "\n",
      "### ORDER OF MODEL: 10 ###\n",
      "### ENTROPY/SYMBOL OF MODEL: 0.5866479159446196 ###\n",
      "### TOTAL VOCABULARY OF MODEL: 3264778 ###\n",
      "### TEST SCORE OF MODEL: 0.3458571564642406, RANDOM BASELINE: 3.062995401218705e-07 ###\n",
      "\n",
      "he incidents, such as no real Queen has ever painted the whole way of speaking to analyse it,\n",
      "imponderable element from myself, painful are those two days later, would in the little train having failed to regard as wholly unnecessary. She had preferred to her, if she could be certain evening, all the rest, simply persecute me to go on your knees to him. And so it can be understood them thoroughly we are at no pains to have some difficult to understand that\n",
      "to seek happiness, who is well-known name. There's no one like that to kiss Albertine or Octave, which was one of those cathedral. It was a concession not of beauty. The anticipation an evening she\n",
      "might have learned that he would again taken, at first, to inform you that my principally in\n",
      "examining the word 'so.' Ignorant at first time that\n",
      "one will make no bones\n",
      "about telling me waste an entirely differently coloured, consummation.\n",
      "The librarian who, having loved by Mlle. Vinteuil, she said to be its author in her place in the past\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for order in [1,2,3,4,5,6,7,8,9,10,15,20]:\n",
    "    mm = Markov('sources/proust_ascii.txt', order, word=False)\n",
    "    print('### ORDER OF MODEL: ' + str(order) + ' ###')\n",
    "    print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "    print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "    print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "    print('\\n' + mm.generate_text(1000) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An order of 6 seems to offer the best test accuracy. A possible explanation could be that a sixth-order character model is probably roughly equivalent to a first-order word model (i.e. the average word length would be ~6). Note that with very high order models, we can almost deterministically describe the text (i.e. the entropy aproaches zero). This makes intuitive sense, as very high order models will have little redundancy in their vocabulary. As a consequence, their test accuracy is also very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 30\n",
    "mm = Markov('sources/proust_ascii.txt', order, word=False)\n",
    "print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "print('\\n' + mm.generate_text(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with another source, Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 5\n",
    "mm = Markov('sources/shakespeare_input.txt', order, word=False)\n",
    "print('### ENTROPY/SYMBOL OF MODEL: ' + str(mm.entropy) + ' ###')\n",
    "print('### TOTAL VOCABULARY OF MODEL: ' + str(len(mm.vocabulary)) + ' ###')\n",
    "print('### TEST SCORE OF MODEL: ' + str(mm.test()) + ', RANDOM BASELINE: ' + str(1/len(mm.vocabulary)) + ' ###')\n",
    "print('\\n' + mm.generate_text(1000) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLLMs and LSTMs\n",
    "\n",
    "- Now we understand why it is important to consider sequences as Markov models, i.e. take the \"history\" of a sequence into account: this gives us the possibility to model a sequence as a set of dependend variables, which enables the model to \"encode\" structure beyond simple symbol ratios. CLLMs and LSTMs are (super-easy and super-complicated, respectively) ways of accomplishing this. \n",
    "- CLLMs exactly follow Shannon: they extract an $n$-th order transition matrix from a source of natural text, and generate new text by simply applying the transition matrix one $n$-gram at a time.\n",
    "- LSTMs apply the idea of states and transitions (i.e. Markov chains) to neural networks.\n",
    "\n",
    "![](img/rnn.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
