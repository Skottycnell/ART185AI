{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2019 Fabian Offert, [MIT License](LICENSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Feature visualization has been an important area of research within machine learning in general and deep learning in particular at least since 2014. \"Deep Dream\", for instance, works by applying feature visualization techniques to images, albeit optimized for producing the kind of visuals it has become famous for. Since then, particularly with the invention of GANs, more elaborate methods have emerged that employ natural image priors to \"bias\" visualizations towards more \"legible\" images. Recently, feature visualization and related methods have received a lot of attention as possible solutions to the problem of interpretability. Nevertheless, almost all visualization methods rely on the principle of activation maximization. They visualize the learned features of a particular neuron/channel/layer by optimizing an input image to maximally activate this neuron/channel/layer. Below, we visualize the features of selected channels from the InceptionV3 network, trained on ImageNet. For the classification layer, impressive visualizations like this are attainable with the default parameters.\n",
    "\n",
    "![](img/banana.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To be able to use mechanisms like differentiation and backpropagation we need a high-level library that abstracts these mathematical details away from us. We are using PyTorch, one of the de-facto standard frameworks for high-level prototyping for machine learning. We are also importing an existing, slightly modified network architecture: InceptionV3. Because we are operating in high-dimensional vector space, we are also using Numpy, the Python library for scientific computing. We are also importing various image filters from the skimage library. Finally, we are importing a bunch of helper functions to render images within the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "\n",
    "from skimage.restoration import denoise_bilateral, denoise_tv_chambolle\n",
    "\n",
    "from inception_debug import inception_v3\n",
    "\n",
    "CUDA = False # Set this to True if using a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "To visualize a network we need to know which layers are available. Here is an overview of the architecture of InceptionV3 (more information [here](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/) and in the [original paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)):\n",
    "    \n",
    "![](img/inception.jpg)\n",
    "\n",
    "How do we know how these layers are called? We can simply load the model, add pre-trained ImageNet weights to it, and iterate over the available layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available modules:\n",
      "0 Conv2d_1a_3x3\n",
      "1 Conv2d_2a_3x3\n",
      "2 Conv2d_2b_3x3\n",
      "3 Conv2d_3b_1x1\n",
      "4 Conv2d_4a_3x3\n",
      "5 Mixed_5b\n",
      "6 Mixed_5c\n",
      "7 Mixed_5d\n",
      "8 Mixed_6a\n",
      "9 Mixed_6b\n",
      "10 Mixed_6c\n",
      "11 Mixed_6d\n",
      "12 Mixed_6e\n",
      "13 AuxLogits\n",
      "14 Mixed_7a\n",
      "15 Mixed_7b\n",
      "16 Mixed_7c\n",
      "17 fc\n"
     ]
    }
   ],
   "source": [
    "mode = None\n",
    "if CUDA:\n",
    "    model = inception_v3(pretrained=True).cuda()\n",
    "else:\n",
    "    model = inception_v3(pretrained=True)\n",
    "\n",
    "model.eval() # Test mode, we are not training anything\n",
    "\n",
    "print('Available modules:')\n",
    "for n, name_module in enumerate(model.named_children()):\n",
    "    print(n, name_module[0])\n",
    "f = model # Alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing and deprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each color sepearately (Photoshop auto tone)\n",
    "def autotone_PIL(img):\n",
    "    img = np.array(img)\n",
    "    img[:,:,0] = np.interp(img[:,:,0], [np.amin(img[:,:,0]), np.amax(img[:,:,0])], [0, 255])\n",
    "    img[:,:,1] = np.interp(img[:,:,1], [np.amin(img[:,:,1]), np.amax(img[:,:,1])], [0, 255])\n",
    "    img[:,:,2] = np.interp(img[:,:,2], [np.amin(img[:,:,2]), np.amax(img[:,:,2])], [0, 255])\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def deprocess(tensor):\n",
    "    # Remove batch dimension\n",
    "    # Shape before: BCSS (batch, channels, size, size)\n",
    "    tensor = tensor.data.squeeze() \n",
    "    # Shape after: CSS\n",
    "        \n",
    "    # To CPU and numpy array\n",
    "    img = tensor.cpu().numpy() \n",
    "    \n",
    "    # Channels last\n",
    "    # Shape before: CSS\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 0, 1)\n",
    "    # Shape after: SSS\n",
    "\n",
    "    # Clip to visible range\n",
    "    img = np.clip(img, 0., 1.) # Clip to 0./1. range\n",
    "    \n",
    "    # 0./1. range to 0./255. range\n",
    "    img *= 255. \n",
    "        \n",
    "    # To PIL image format\n",
    "    img = PIL.Image.fromarray(img.astype(np.uint8))  \n",
    "    \n",
    "    return img\n",
    "\n",
    "def gray_square_PIL(size):\n",
    "    # Gray square, -1./1. range\n",
    "    img = np.random.normal(0, 0.01, (size, size, 3)) \n",
    "    \n",
    "    # -1./1. range to 0./255. range\n",
    "    img /= 2.\n",
    "    img += 0.5\n",
    "    img *= 255.\n",
    "\n",
    "    # To PIL image format\n",
    "    img = PIL.Image.fromarray(img.astype(np.uint8))\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def show_img_PIL(img, fmt='jpeg'):\n",
    "    f = BytesIO()\n",
    "    img.save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "    \n",
    "def pytorch_to_skimage(img):\n",
    "    # No batch dimension\n",
    "    img = img[0]\n",
    "    # Channels last\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    return img\n",
    "    \n",
    "def skimage_to_pytorch(img):\n",
    "    # Channels first\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    # Skimage uses double\n",
    "    img = img.astype(np.float32)\n",
    "    # No Batch dimension\n",
    "    img = np.expand_dims(img, 0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 300 # Default 300, very good: 2000\n",
    "FILTER = 20 # TV filter frequency, default 20\n",
    "JITTER = 32 # Jitter max. offset, default 32\n",
    "LR = 0.4 # Learning rate, default 0.4\n",
    "L2 = 1e-4 # L2 regularization, default 1e-4\n",
    "OCTAVES = 3 # Octaves, default 3\n",
    "SCALE = 1.5 # Octave scale, default 1.5\n",
    "ORIGINAL_SIZE = 299 # Inception V3, do not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient ascent function\n",
    "\n",
    "The optimization tricks are compiled from various papers. This is an experimentally verified optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(img, layer, neuron, shape, predictions):\n",
    "            \n",
    "    # Pytorch is channels first, this happens here!\n",
    "    preprocess = tv.transforms.Compose([tv.transforms.Resize(shape), tv.transforms.ToTensor()])\n",
    "    \n",
    "    # unsqueeze(0) adds batch dimension\n",
    "    # cuda() puts on GPU\n",
    "    # requires_grad_() switches on gradients (gradients for the model are switched off in model.eval())\n",
    "    input = None\n",
    "    if CUDA:\n",
    "        input = preprocess(img).unsqueeze(0).cuda().requires_grad_()\n",
    "    else:\n",
    "        input = preprocess(img).unsqueeze(0).requires_grad_()\n",
    "    \n",
    "    # Define the stochastic gradient descent optimizer\n",
    "    optimizer = t.optim.SGD([input], lr=LR, weight_decay=L2)\n",
    "    \n",
    "    # Optimize\n",
    "    for i in range(ITERATIONS):\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Jitter on\n",
    "        if JITTER:\n",
    "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "            ox, oy = np.random.randint(-JITTER, JITTER+1, 2)\n",
    "            npimg = np.roll(np.roll(npimg, ox, -1), oy, -2) # Jitter\n",
    "            if CUDA:\n",
    "                input.data = t.from_numpy(npimg).cuda()\n",
    "            else:\n",
    "                input.data = t.from_numpy(npimg)\n",
    "        \n",
    "        # Custom forward() function in inception_debug.py to keep all the extra stuff from the default\n",
    "        # forward() function while going through the model layer by layer\n",
    "        x = f.forward_layer(input, layer) # Forward pass up to the feature extractor layer\n",
    "        \n",
    "        # Prediction layer\n",
    "        if predictions:\n",
    "            loss = -x[:,neuron]\n",
    "        \n",
    "        # Other layer\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.norm,\n",
    "        # equivalent to Keras K.maximum(K.mean(K.abs(x)), K.epsilon())\n",
    "        else:\n",
    "            loss = -x[:,neuron].norm()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Jitter off\n",
    "        if JITTER:\n",
    "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "            npimg = np.roll(np.roll(npimg, -ox, -1), -oy, -2) # Jitter\n",
    "            if CUDA:\n",
    "                input.data = t.from_numpy(npimg).cuda()\n",
    "            else:\n",
    "                input.data = t.from_numpy(npimg)\n",
    "            \n",
    "        # Stochastic clipping\n",
    "        input.data[input.data > 1] = np.random.uniform(0, 1)\n",
    "        input.data[input.data < 0] = np.random.uniform(0, 1)\n",
    "        \n",
    "        # TV filtering\n",
    "        if i != ITERATIONS - 1: # No regularization on last iteration for good quality output\n",
    "            if i % FILTER == 0:\n",
    "                npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "                npimg = pytorch_to_skimage(npimg)\n",
    "                npimg = denoise_tv_chambolle(npimg, weight=0.1, multichannel=True)\n",
    "                npimg = skimage_to_pytorch(npimg)\n",
    "                if CUDA:\n",
    "                    input.data = t.from_numpy(npimg).cuda()\n",
    "                else:\n",
    "                    input.data = t.from_numpy(npimg)\n",
    "\n",
    "    img = deprocess(input)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(layer, neuron, predictions):\n",
    "    \n",
    "    # Prepare octaves, they are the same for all neurons/layers\n",
    "    original_shape = (ORIGINAL_SIZE, ORIGINAL_SIZE)\n",
    "    successive_shapes = [original_shape]\n",
    "    for i in range(1, OCTAVES):\n",
    "        hw = int(successive_shapes[i-1][0] * SCALE)\n",
    "        successive_shapes.append((hw, hw))\n",
    "    final_shape =  successive_shapes[-1]\n",
    "\n",
    "    # Input noise image\n",
    "    img = gray_square_PIL(ORIGINAL_SIZE)\n",
    "    \n",
    "    # Excplicitly copy noise image\n",
    "    final_img = img.copy()\n",
    "\n",
    "    # Do octaves\n",
    "    shrunk_final_img = img.resize(successive_shapes[0], PIL.Image.ANTIALIAS)\n",
    "    for octave, shape in enumerate(successive_shapes):\n",
    "        \n",
    "        print('layer', layer, 'neuron', neuron, 'octave', octave+1, '/', OCTAVES, 'shape', shape)\n",
    "\n",
    "        img = gradient_ascent(img, layer, neuron, shape, predictions)\n",
    "\n",
    "        # Resize (\"upscale\") previous shape original image to current shape\n",
    "        upscaled_shrunk_final_img = shrunk_final_img.resize(shape, PIL.Image.ANTIALIAS)\n",
    "\n",
    "        # Resize (\"downscale\") original shape original image to current shape\n",
    "        same_size_final_img = final_img.resize(shape, PIL.Image.ANTIALIAS)\n",
    "\n",
    "        # Find the details that are lost in the upscaling process (in numpy space)\n",
    "        lost_detail = np.array(same_size_final_img) - np.array(upscaled_shrunk_final_img)\n",
    "\n",
    "        # Add these details back to the optimized image (in numpy space)\n",
    "        img = np.array(img) + lost_detail\n",
    "\n",
    "        # To PIL\n",
    "        img = PIL.Image.fromarray(img)\n",
    "\n",
    "        # Prepare for next ocatve\n",
    "        shrunk_final_img = final_img.resize(shape, PIL.Image.ANTIALIAS)\n",
    "\n",
    "        # Show with autotone\n",
    "        show_img_PIL(autotone_PIL(img))\n",
    "        \n",
    "    img = autotone_PIL(img)      \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synset_words.txt') as synset_words_file:\n",
    "    synset_words = synset_words_file.readlines()\n",
    "\n",
    "for i, line in enumerate(synset_words):\n",
    "    synset_words[i] = line.replace(' ', '_').replace(',', '_').lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ = 987\n",
    "print(class_, synset_words[class_])\n",
    "img = generate(17, class_, predictions=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
